#Hands on Machine-learning with Scikit-Learn and TensorFlow
## Chapter 1 - Intro
In *supervised learning* the training set is composed of labelled instances, labels help the algorithm to classify instances based on the feature the label describes. Example of a usual supervised approaches are classification and regression. Supervised learning algorithms include:
- k-Nearest Neighbors
  - Linear Regression
    -Logistic regression
    -SVM
    -Decision trees and random forests
    -some Neural nets

    Vice versa, in *unsupervised learning*, the training instances are not labelled, therefore the algorithms try to learn without being taught. Some uns. learning algorithms:

    -k-Means
    -Hierarchical clustering
    -PCA
    -Kernel PCA
    -Locally-linear embedding
    -t-SNE 

    Unsupervised learning allows for _feature extraction_, the process by which a dimensionality reduction algorithm will highlight higly correlated features and merge into one to decrease dimensionality. Another useful aspect is anomaly detection (much like looking at outlier samples on a PCA).

    In *Reinforcement Learning*, an agent is trained to observe the environment it is inserted in and then let free to act, for each action then a penalty or reward is attributed and this is how the algorithm learns. An example of this is AlphaGO. The algorithm was enhanced by making it play against itself and rewarding it for every win.

    *Batch learning* refers to the fact that an algorithm is trained once with all the available data _offline_ and then it is launched into production as is. This means that to update the performace of the system for new data, a new trained version has to be released. These type of algorithms are uncapable of learning _incrementally_.

    In *online learning* the problem is circumvented by feeding the system with data instances _sequentially_ in mini-batches which makes each learning step fast and cheap. Even if online is in the name, this whole process is done offline and not in production, the only thing changing is the amount and frequency at which training data is fed to the system.

    One of the main paradigms of ML systems is _generalization_, meaning the ability to perform on data never seen before and different from the original training set. The two main approaches to generalization are *Instance-based Learning* and *Model-based Learning*.
    In instance-based learning, the system generalizes by measuring feature similarity between the data it has already seen and the new one. Another way to generalize is to make a _model_ which is then used to make _predictions_, this is known as model-based learning.

    ### What could go wrong in learning?
    - Bad Data
      Avoid small amounts of training data, it was demonstrated that training on bigger datasets equally improved the  performance of simpler algorithms. Avoid training on data which is not representative of what you are trying to predict, obviously this will lead to a model with lower predictive capability. This last indication can be tricky to accomplish as large training data set might be _noisy_ (lots of instances are non-representative due to chance) and/or conceal _sampling biases_ when too small.
      Additioally, cleaning data is a must, avoiding features with a high percentage of missing instances and/or filling them in is crucial as is detecting outliers that might skew predictions. The challenge is to _engineer_ features to train the system on, selecting the most valuable ones or combining them (_feature extraction_) to achieve a more useful one (with the help of dimensionality reduction).

      - Bad Algorithm applications
        Whenever a model performs well on the training data but does not generalize, it is _overfitted_. This might happen whenever the model is too complex compared to the amount of noise in the training data. Fixes include reducing model complexity, gather more training data or select different features to train on and/ore reduce noise (clean-up).
        Another option is to _constrain_ the model, this procedure is called *regularization* and it entails the reduction of the degrees of freedom available to the model to fit the data through limitations on the prarameters. Regularization during learning can be controlled by a _hyperparameter_, which is a parameter of the learning algorithm (not of the model!). Therefore it remains constant during training and needs to be set prior to training.
        The opposite scenario is called _underfitting_, meaning that the model is too simple and has trouble learning the underlying structure of the data.

        - The role of Testing and Validating
          Usually, data used to create and optimize a ML model is split into two sets, a _training set_, which has to fit all the above-mentioned cases and a _test set_, which is a representation of the future data the model will encounter and is used to assess its validity. It is common to use a 80/20 split between training and test data.
          The error rate on new instances seen is known as _generalization error_. Whenever the trainin error (i.e. wrong predictions during training) is low but the generalization error is high, it means that the model is overfitting the training data. A best practice, especially when multiple-testing different models and/or tuning hyperparameters, is to hold out an additional subset of the training data known as _validation set_. This is only touched the moment we feel our model is the best performing one on the test set, to understand its ability to generalize to unseen data. To avoid reducing the training set too much by splitting into test and validation, a common technique is _cross validation_, where the training set is split into many different complementary subsets and each model is trained on a combination of these subsets and validated against the remaining parts.
    
